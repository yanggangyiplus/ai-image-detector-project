#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
- í˜¼ë™ í–‰ë ¬ ìƒì„±
- ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„±
- ROC ê³¡ì„  ë° AUC ê³„ì‚°
"""

import os
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
from transformers import pipeline, ViTForImageClassification, ViTImageProcessor
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

class ModelEvaluator:
    """AI ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path='./ai_vs_real_image_detection'):
        """
        ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path (str): í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ
        """
        self.model_path = model_path
        self.device = 0 if torch.cuda.is_available() else -1
        
        # ëª¨ë¸ ë¡œë“œ
        print("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        try:
            self.classifier = pipeline(
                'image-classification',
                model=model_path,
                device=self.device
            )
            self.model = ViTForImageClassification.from_pretrained(model_path)
            self.processor = ViTImageProcessor.from_pretrained(model_path)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ë””ë°”ì´ìŠ¤: {'GPU' if self.device == 0 else 'CPU'})")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def load_test_data(self, test_data_path='test data'):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        
        Args:
            test_data_path (str): í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            tuple: (images, labels) ë¦¬ìŠ¤íŠ¸
        """
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        images = []
        labels = []
        
        # AI ìƒì„± ì´ë¯¸ì§€ (label=1) - fake í´ë”
        ai_path = Path(test_data_path) / 'fake'
        if ai_path.exists():
            for img_file in ai_path.glob('*'):
                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']:
                    try:
                        image = Image.open(img_file).convert('RGB')
                        images.append(image)
                        labels.append(1)  # AI ìƒì„±
                        print(f"   AI ì´ë¯¸ì§€ ë¡œë“œ: {img_file.name}")
                    except Exception as e:
                        print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file} - {e}")
        
        # ì‹¤ì œ ì´ë¯¸ì§€ (label=0) - real í´ë”
        real_path = Path(test_data_path) / 'real'
        if real_path.exists():
            for img_file in real_path.glob('*'):
                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']:
                    try:
                        image = Image.open(img_file).convert('RGB')
                        images.append(image)
                        labels.append(0)  # ì‹¤ì œ
                        print(f"   ì‹¤ì œ ì´ë¯¸ì§€ ë¡œë“œ: {img_file.name}")
                    except Exception as e:
                        print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file} - {e}")
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(images)}ê°œ ì´ë¯¸ì§€")
        print(f"   - AI ìƒì„±: {labels.count(1)}ê°œ")
        print(f"   - ì‹¤ì œ: {labels.count(0)}ê°œ")
        
        return images, labels
    
    def predict_batch(self, images, batch_size=32):
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            images (list): ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            
        Returns:
            tuple: (predictions, probabilities)
        """
        print("ğŸ”® ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        
        all_predictions = []
        all_probabilities = []
        
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i+batch_size]
            print(f"   ë°°ì¹˜ {i//batch_size + 1}/{(len(images)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")
            
            for image in batch_images:
                try:
                    # ì˜ˆì¸¡ ìˆ˜í–‰
                    result = self.classifier(image)
                    
                    # ê²°ê³¼ íŒŒì‹±
                    prediction = 1 if result[0]['label'] == 'AI_GENERATED' else 0
                    probability = result[0]['score']
                    
                    all_predictions.append(prediction)
                    all_probabilities.append(probability)
                    
                except Exception as e:
                    print(f"âš ï¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    all_predictions.append(0)  # ê¸°ë³¸ê°’
                    all_probabilities.append(0.5)
        
        print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(all_predictions)}ê°œ")
        return all_predictions, all_probabilities
    
    def calculate_metrics(self, y_true, y_pred, y_prob):
        """
        ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        
        Args:
            y_true (list): ì‹¤ì œ ë ˆì´ë¸”
            y_pred (list): ì˜ˆì¸¡ ë ˆì´ë¸”
            y_prob (list): ì˜ˆì¸¡ í™•ë¥ 
            
        Returns:
            dict: ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        
        # ê¸°ë³¸ ì§€í‘œ
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='weighted')
        recall = recall_score(y_true, y_pred, average='weighted')
        f1 = f1_score(y_true, y_pred, average='weighted')
        
        # ROC AUC
        try:
            roc_auc = roc_auc_score(y_true, y_prob)
        except:
            roc_auc = 0.0
        
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'total_samples': len(y_true),
            'ai_generated_correct': sum(1 for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true == 1 and pred == 1),
            'real_correct': sum(1 for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true == 0 and pred == 0),
            'ai_generated_total': sum(y_true),
            'real_total': len(y_true) - sum(y_true)
        }
        
        return metrics
    
    def plot_confusion_matrix(self, y_true, y_pred, save_path='results/confusion_matrix.png'):
        """
        í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        
        Args:
            y_true (list): ì‹¤ì œ ë ˆì´ë¸”
            y_pred (list): ì˜ˆì¸¡ ë ˆì´ë¸”
            save_path (str): ì €ì¥ ê²½ë¡œ
        """
        print("ğŸ“ˆ í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")
        
        # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        cm = confusion_matrix(y_true, y_pred)
        
        # ì‹œê°í™”
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['ì‹¤ì œ', 'AI ìƒì„±'],
                   yticklabels=['ì‹¤ì œ', 'AI ìƒì„±'])
        plt.title('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)')
        plt.xlabel('ì˜ˆì¸¡ ë ˆì´ë¸”')
        plt.ylabel('ì‹¤ì œ ë ˆì´ë¸”')
        
        # ì €ì¥
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… í˜¼ë™ í–‰ë ¬ ì €ì¥: {save_path}")
    
    def plot_roc_curve(self, y_true, y_prob, save_path='results/roc_curve.png'):
        """
        ROC ê³¡ì„  ì‹œê°í™”
        
        Args:
            y_true (list): ì‹¤ì œ ë ˆì´ë¸”
            y_prob (list): ì˜ˆì¸¡ í™•ë¥ 
            save_path (str): ì €ì¥ ê²½ë¡œ
        """
        print("ğŸ“ˆ ROC ê³¡ì„  ìƒì„± ì¤‘...")
        
        try:
            # ROC ê³¡ì„  ê³„ì‚°
            fpr, tpr, _ = roc_curve(y_true, y_prob)
            roc_auc = roc_auc_score(y_true, y_prob)
            
            # ì‹œê°í™”
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkorange', lw=2, 
                    label=f'ROC ê³¡ì„  (AUC = {roc_auc:.3f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                    label='ë¬´ì‘ìœ„ ë¶„ë¥˜ê¸°')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('ROC ê³¡ì„  (Receiver Operating Characteristic)')
            plt.legend(loc="lower right")
            plt.grid(True, alpha=0.3)
            
            # ì €ì¥
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… ROC ê³¡ì„  ì €ì¥: {save_path}")
            
        except Exception as e:
            print(f"âš ï¸ ROC ê³¡ì„  ìƒì„± ì‹¤íŒ¨: {e}")
    
    def generate_classification_report(self, y_true, y_pred, save_path='results/classification_report.txt'):
        """
        ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            y_true (list): ì‹¤ì œ ë ˆì´ë¸”
            y_pred (list): ì˜ˆì¸¡ ë ˆì´ë¸”
            save_path (str): ì €ì¥ ê²½ë¡œ
        """
        print("ğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„±
        report = classification_report(y_true, y_pred, 
                                    target_names=['ì‹¤ì œ', 'AI ìƒì„±'],
                                    output_dict=True)
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
        text_report = classification_report(y_true, y_pred, 
                                          target_names=['ì‹¤ì œ', 'AI ìƒì„±'])
        
        # ì €ì¥
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write("AI ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"í‰ê°€ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(y_true)}\n\n")
            f.write(text_report)
        
        print(f"âœ… ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì €ì¥: {save_path}")
        return report
    
    def save_evaluation_results(self, metrics, save_path='results/evaluation_results.json'):
        """
        í‰ê°€ ê²°ê³¼ ì €ì¥
        
        Args:
            metrics (dict): ì„±ëŠ¥ ì§€í‘œ
            save_path (str): ì €ì¥ ê²½ë¡œ
        """
        print("ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        results = {
            'evaluation_time': datetime.now().isoformat(),
            'model_path': self.model_path,
            'device': 'GPU' if self.device == 0 else 'CPU',
            'metrics': metrics
        }
        
        # ì €ì¥
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥: {save_path}")
    
    def print_summary(self, metrics):
        """
        í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        
        Args:
            metrics (dict): ì„±ëŠ¥ ì§€í‘œ
        """
        print("\n" + "="*60)
        print("ğŸ¯ AI ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ í‰ê°€ ê²°ê³¼")
        print("="*60)
        
        print(f"ğŸ“Š ì „ì²´ ì„±ëŠ¥:")
        print(f"   ì •í™•ë„ (Accuracy): {metrics['accuracy']:.3f}")
        print(f"   ì •ë°€ë„ (Precision): {metrics['precision']:.3f}")
        print(f"   ì¬í˜„ìœ¨ (Recall): {metrics['recall']:.3f}")
        print(f"   F1 ì ìˆ˜: {metrics['f1_score']:.3f}")
        print(f"   ROC AUC: {metrics['roc_auc']:.3f}")
        
        print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        print(f"   AI ìƒì„± ì´ë¯¸ì§€:")
        print(f"     - ì •í™•ë„: {metrics['ai_generated_correct']}/{metrics['ai_generated_total']} ({metrics['ai_generated_correct']/metrics['ai_generated_total']:.3f})")
        print(f"   ì‹¤ì œ ì´ë¯¸ì§€:")
        print(f"     - ì •í™•ë„: {metrics['real_correct']}/{metrics['real_total']} ({metrics['real_correct']/metrics['real_total']:.3f})")
        
        print(f"\nğŸ“‹ ì´ ìƒ˜í”Œ ìˆ˜: {metrics['total_samples']}")
        print("="*60)
    
    def evaluate_model(self, test_data_path='data/test', output_dir='results'):
        """
        ì „ì²´ ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
        
        Args:
            test_data_path (str): í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            output_dir (str): ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        print("ğŸš€ AI ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ í‰ê°€ ì‹œì‘!")
        print("="*60)
        
        try:
            # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            images, labels = self.load_test_data(test_data_path)
            
            if len(images) == 0:
                print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 2. ì˜ˆì¸¡ ìˆ˜í–‰
            predictions, probabilities = self.predict_batch(images)
            
            # 3. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            metrics = self.calculate_metrics(labels, predictions, probabilities)
            
            # 4. ì‹œê°í™” ìƒì„±
            self.plot_confusion_matrix(labels, predictions, 
                                    f'{output_dir}/confusion_matrix.png')
            self.plot_roc_curve(labels, probabilities, 
                              f'{output_dir}/roc_curve.png')
            
            # 5. ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_classification_report(labels, predictions, 
                                              f'{output_dir}/classification_report.txt')
            
            # 6. ê²°ê³¼ ì €ì¥
            self.save_evaluation_results(metrics, f'{output_dir}/evaluation_results.json')
            
            # 7. ê²°ê³¼ ì¶œë ¥
            self.print_summary(metrics)
            
            print(f"\nğŸ‰ ëª¨ë¸ í‰ê°€ ì™„ë£Œ! ê²°ê³¼ëŠ” '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤– AI ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸")
    print("="*60)
    
    # ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = ModelEvaluator()
    
    # ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
    evaluator.evaluate_model(
        test_data_path='test data',
        output_dir='results'
    )

if __name__ == "__main__":
    main()
